summary(NOX_pt)
summary(SO2_pt)
summary(CLIMATE)
summary(AGRICULTURE)
fivenum(EPI, na.rm=TRUE)
#### Central Tendency Values ####
summary(EPI)
fivenum(EPI)
help("summary")
help(fivenum)
E <- EPI[!tf]
E
sum(is.na(E))
sum(is.na(EPI))
data_EPI <- read.csv(skip=0,"C:/Users/Naran/Downloads/2010EPI_data.csv", header=TRUE)
attach(data_EPI)
sum(is.na(EPI))
#### Filtering NAs ####
sum(is.na(DALY))
#### Filtering NAs ####
sum(is.na(CLIMATE))
#### Filtering NAs ####
sum(is.na(EPI))
boxplot(WQI_pt)
#### Boxplot ####
View(data_EPI)
hist(WQI_pt)
plot(WQI_pt)
barplot(WQI_pt)
boxplot(EPI)
boxplot(DALY)
boxplot(OZONE_pt)
boxplot(FISHERIES_pt)
boxplot(NMVOC_pt)
boxplot(FISHERIES_pt)
boxplot(FISHERIES)
boxplot(ENVHEALTH, ECOSYSTEM)
qqplot(ENVHEALTH, ECOSYSTEM)
EPI_data <- read.csv("C:/Users/Naran/Downloads/EPI_data.csv", header=TRUE)
#### Linear Regression Model ####
EPI_data <- read.csv("C:/Users/Naran/Downloads/EPI_data.csv", header=TRUE)
boxplot(EPI_data$ENVHEALTH, EPI_data$DALY ,
EPI_data$AIR_H , EPI_data$WATER_H)
lmENVH <- lm(ENVHEALTH ~ EPI_data$DALY + EPI_data$AIR_H , EPI_data$WATER_H )
lmENVH <- lm(EPI_data$ENVHEALTH ~ EPI_data$DALY + EPI_data$AIR_H , EPI_data$WATER_H )
lmENVH <- lm(EPI_data$ENVHEALTH ~ EPI_data$DALY + EPI_data$AIR_H + EPI_data$WATER_H )
lmENVH
summary(lmENVH)
cENCH <- coef(lmENVH)
#### Prediction ####
DALTNEW <- c(seq(5,95,5))
AIR_HNEW <- c(seq(5,95,5))
WATER_HNEW <- c(seq(5,95,5))
NEW <- data.frame(DALYNEW, AIR_HNEW, WATER_HNEW)
#### Prediction ####
DALYNEW <- c(seq(5,95,5))
AIR_HNEW <- c(seq(5,95,5))
WATER_HNEW <- c(seq(5,95,5))
NEW <- data.frame(DALYNEW, AIR_HNEW, WATER_HNEW)
pENV <- predict(lmENVH, NEW, interval="prediction")
cENV <- predict(lmENVH, NEW, interval="confidence")
#### Prediction ####
DALYNEW <- c(seq(1,95,0.38))
len(lmENVH)
size(lmENVH)
help(size)
help("array")
dim(lmENVH)
dim(lmENVH)
dim(NEW)
lmENVH <- lm(EPI_data$ENVHEALTH ~ EPI_data$DALY + EPI_data$AIR_H + EPI_data$WATER_H )
lmENVH
summary(lmENVH)
cENCH <- coef(lmENVH)
#### Prediction ####
DALYNEW <- c(seq(5,95,5))
AIR_HNEW <- c(seq(5,95,5))
WATER_HNEW <- c(seq(5,95,5))
NEW <- data.frame(DALYNEW, AIR_HNEW, WATER_HNEW)
dim(NEW)
pENV <- predict(lmENVH, NEW, interval="prediction")
cENV <- predict(lmENVH, NEW, interval="confidence")
pENV
help(lm)
summary(EPI_data$ENVHEALTH)
data <- data.frame(Env = EPI_data$ENVHEALTH,
Daly = EPI_data$DALY,
Air_H = EPI_data$AIR_H,
Water_H = EPI_data$WATER_H)
data
data$Env[is.na(data$Env)] <- mean(data$Env, na.rm = TRUE)
data$Daly[is.na(data$Daly)] <- mean(data$Daly, na.rm = TRUE)
data$Air_H[is.na(data$Air_H)] <- mean(data$Air_H, na.rm = TRUE)
data$Env[is.na(data$Water_H)] <- mean(data$Water_H, na.rm = TRUE)
data
data$Env[is.na(data$Env)] <- mean(data$Env, na.rm = TRUE)
data$Daly[is.na(data$Daly)] <- mean(data$Daly, na.rm = TRUE)
data$Air_H[is.na(data$Air_H)] <- mean(data$Air_H, na.rm = TRUE)
data$Water_H[is.na(data$Water_H)] <- mean(data$Water_H, na.rm = TRUE)
data
lmENVH <- lm(data$ENVHEALTH ~ data$DALY + data$AIR_H + data$WATER_H )
boxplot(data$Env, data$Daly ,
data$Air_H , data$Water_H)
lmENVH <- lm(data$Env ~ data$Daly + data$Air_H + data$Water_H )
lmENVH
summary(lmENVH)
cENCH <- coef(lmENVH)
#### Prediction ####
DALYNEW <- c(seq(5,95,5))
AIR_HNEW <- c(seq(5,95,5))
WATER_HNEW <- c(seq(5,95,5))
NEW <- data.frame(DALYNEW, AIR_HNEW, WATER_HNEW)
dim(NEW)
pENV <- predict(lmENVH, NEW, interval="prediction")
cENV <- predict(lmENVH, NEW, interval="confidence")
pENV
cENV
#### Linear Model Using AIR_E ####
data <- data.frame(Env = EPI_data$ENVHEALTH,
Daly = EPI_data$DALY,
Air_E = EPI_data$AIR_E,
Water_H = EPI_data$WATER_E)
#Replacing all NA values with the mean of the column
data$Env[is.na(data$Env)] <- mean(data$Env, na.rm = TRUE)
data$Daly[is.na(data$Daly)] <- mean(data$Daly, na.rm = TRUE)
data$Air_E[is.na(data$Air_E)] <- mean(data$Air_E, na.rm = TRUE)
data$Water_H[is.na(data$Water_H)] <- mean(data$Water_H, na.rm = TRUE)
boxplot(data$Env, data$Daly ,
data$Air_E , data$Water_H)
#### Linear Model ####
lmENVH <- lm(data$Env ~ data$Daly + data$Air_E + data$Water_H )
lmENVH
summary(lmENVH)
cENCH <- coef(lmENVH)
#### Prediction ####
DALYNEW <- c(seq(5,95,5))
AIR_ENEW <- c(seq(5,95,5))
WATER_HNEW <- c(seq(5,95,5))
NEW <- data.frame(DALYNEW, AIR_ENEW, WATER_HNEW)
dim(NEW)
pENV <- predict(lmENVH, NEW, interval="prediction")
cENV <- predict(lmENVH, NEW, interval="confidence")
pENV
cENV
#### Linear Model Using Climate as the dependent ####
data <- data.frame(Climate = EPI_data$CLIMATE,
Daly = EPI_data$DALY,
Air_H = EPI_data$AIR_H,
Water_H = EPI_data$WATER_E)
boxplot(data$Climate, data$Daly ,
data$Air_H , data$Water_H)
#### Linear Model ####
lmClimate <- lm(data$Climate ~ data$Daly + data$Air_H + data$Water_H )
lmClimate
summary(lmClimate)
cENCH <- coef(lmClimate)
#### Prediction ####
DALYNEW <- c(seq(5,95,5))
AIR_ENEW <- c(seq(5,95,5))
WATER_HNEW <- c(seq(5,95,5))
NEW <- data.frame(DALYNEW, AIR_ENEW, WATER_HNEW)
dim(NEW)
pENV <- predict(lmClimate, NEW, interval="prediction")
cENV <- predict(lmClimate, NEW, interval="confidence")
pENV
cENV
data <- read.csv("C:/Users/Naran/Downloads/dataset_multipleRegression.csv")
df <- data.frame(Unem = data$UNEM,
Hgrad = data$HGRAD,
Roll = data$ROLL)
sum(is.na(df$Unem))
sum(is.na(df$Hgrad))
sum(is.na(df$Roll))
df
[1,2]
(1,2)
(1 2 )
(1 2 4)
lmRoll <- lm(df$Roll ~ df$Unem + df$Hgrad)
pRoll <- predict(lmRoll, c(7,90000), interval="predict")
conditions <- data.frame(7,90000)
pRoll <- predict(lmRoll, conditions, interval="predict")
pRoll
df <- data.frame(Unem = data$UNEM,
Hgrad = data$HGRAD,
Roll = data$ROLL,
Inc = data$INC)
sum(is.na(df$Inc))
lmRoll <- lm(Roll ~ Unem + Hgrad, data = df)
conditions <- data.frame(7,90000)
pRoll <- predict(lmRoll, conditions, interval="predict")
pRoll
lmRoll <- lm(Roll ~ Unem + Hgrad, data = df)
conditions <- data.frame(7,90000)
pRoll <- predict(lmRoll, conditions, interval="predict")
conditions <- data.frame(c(7),c(90000))
pRoll <- predict(lmRoll, conditions, interval="predict")
pRoll
conditions <- data.frame(Unem = c(7), Hgrad = c(90000))
pRoll <- predict(lmRoll, conditions, interval="predict")
pRoll
df
lmRoll <- lm(Roll ~ Unem + Hgrad + Inc, data = df)
conditions <- data.frame(Unem = c(7), Hgrad = c(90000), Inc = (25000))
pRoll <- predict(lmRoll, conditions, interval="predict")
pRoll
set.seed(12345)
par(mar = rep(0.2,4))
data_matrix <- matrix(rnorm(400), nrow = 40)
image(1:10, 1:40, t(data_matrix)[,nrow(data_matrix):1])
heatmap(data_matrix)
help(rbinom)
help(heatmap)
set.seed(678910)
for(i in 1:40){
coin_Flip <- rbinom(1, size = 1, prob = 0.5)
if(coin_Flip){
data_matrix[i,] <- data_matrix[i,] + rep(c(0,3), each = 5)
}
}
set.seed(678910)
for(i in 1:40){
coin_Flip <- rbinom(1, size = 1, prob = 0.5)
if(coin_Flip){
print(coin_Flip)
data_matrix[i,] <- data_matrix[i,] + rep(c(0,3), each = 5)
}
}
set.seed(678910)
for(i in 1:40){
coin_Flip <- rbinom(1, size = 1, prob = 0.5)
if(coin_Flip){
print(coin_Flip)
data_matrix[i,] <- data_matrix[i,] + rep(c(0,3), each = 5)
}
}
set.seed(678910)
for(i in 1:40){
coin_Flip <- rbinom(1, size = 1, prob = 0.5)
if(coin_Flip){
print(coin_Flip)
data_matrix[i, ] <- data_matrix[i, ] + rep(c(0,3), each = 5)
}
}
heatmap(data_matrix)
image(1:10, 1:40, t(data_Matrix)[, nrow(data_Matrix):1])
image(1:10, 1:40, t(data_matrix)[, nrow(data_matrix):1])
heatmap(data_matrix)
hh <- hclust(dist(data_matrix))
data_matrix_ordered <= data_matrix[hh$order,]
par(mfrow = c(1,3))
hh <- hclust(dist(data_matrix))
data_matrix_ordered <- data_matrix[hh$order,]
par(mfrow = c(1,3))
image(t(data_matrix_ordered)[, nrow(data_matrix_ordered):1])
plot(rowMeans(data_matrix_ordered), 40:1, , xlab="The Row Mean", ylab = "Row", pch19)
plot(colMeans(data_matrix_ordered), 40:1, , xlab="Column", ylab = "Column Mean", pch19)
plot(rowMeans(data_matrix_ordered), 40:1, , xlab="The Row Mean", ylab = "Row", pch =19)
plot(colMeans(data_matrix_ordered), 40:1, , xlab="Column", ylab = "Column Mean", pch=19)
library(gdata)
#faster xls reader but requires perl!
bronx1<-read.xls(file.choose(),pattern="BOROUGH",stringsAsFactors=FALSE,sheet=1,perl="<SOMEWHERE>/perl/bin/perl.exe")
bronx1<-bronx1[which(bronx1$GROSS.SQUARE.FEET!="0" & bronx1$LAND.SQUARE.FEET!="0" & bronx1$SALE.PRICE!="$0"),]
pairs(~ Fertility + Education + Catholic, data = swiss, subset = Education < 20, main = "Swiss data, Education < 20")
require(party)
swiss_ctree <- ctree(Fertility ~ Agriculture + Education + Catholic, data = swiss)
require(party)
swiss_ctree <- ctree(Fertility ~ Agriculture + Education + Catholic, data = swiss)
plot(swiss_ctree)
# read data in
nyt1<-read.csv("nyt1.csv")
# read data in
nyt1<-read.csv("nyt1.csv/")
# read data in
nyt1<-read.csv("./nyt1.csv/")
dir
path
# read data in
nyt1<-read.csv("nyt1.csv")
help(read/topic = )
help("read.csv")
# read data in
nyt1<-read.csv("nyt1.csv", header=TRUE)
# read data in
nyt1<-read.csv("C:/Users/Naran/DataAnalytics2022_Alejandro_Naranjo/Group2_Scripts/Lab2", header=TRUE)
require(kknn)
data(ionosphere)
# Creating the model using the training and testing set
fit.kknn <- kknn(class ~ ., ionosphere.learn, ionosphere.valid)
(kknn)
require(kknn)
data(ionosphere)
#Splitting the data into a training/testing
ionosphere.learn <- ionosphere[1:200,]
ionosphere.valid <- ionosphere[-c(1:200),]
# Creating the model using the training and testing set
fit.kknn <- kknn(class ~ ., ionosphere.learn, ionosphere.valid)
table(ionosphere.valid$class, fit.kknn$fit)
# Training the model using the training set using a distance of 1
(fit.train1 <- train.kknn(class ~ ., ionosphere.learn, kmax = 15,
kernel = c("triangular", "rectangular", "epanechnikov", "optimal"), distance = 1))
table(predict(fit.train1, ionosphere.valid), ionosphere.valid$class)
(fit.train2 <- train.kknn(class ~ ., ionosphere.learn, kmax = 15,
kernel = c("triangular", "rectangular", "epanechnikov", "optimal"), distance = 2))
table(predict(fit.train2, ionosphere.valid), ionosphere.valid$class)
data(iris)
m <- dim(iris)[1]
val <- sample(1:m, size = round(m/3), replace = FALSE,
prob = rep(1/m, m))
#Splitting the data into train and test
iris.learn <- iris[-val,]
iris.valid <- iris[val,]
iris.kknn <- kknn(Species~., iris.learn, iris.valid, distance = 1,
kernel = "triangular")
summary(iris.kknn)
fit <- fitted(iris.kknn)
table(iris.valid$Species, fit)
pcol <- as.character(as.numeric(iris.valid$Species))
pairs(iris.valid[1:4], pch = pcol, col = c("green3", "red")
[(iris.valid$Species != fit)+1])
install.packages("car")
require(car)
scatterplotMatrix(iris)
# and
scatterplotMatrix(swiss)
scatterplotMatrix(iris)
# and
scatterplotMatrix(swiss)
require(lattice)
super.sym <- trellis.par.get("superpose.symbol")
splom(~iris[1:4], groups = Species, data = iris,
panel = panel.superpose,
key = list(title = "Three Varieties of Iris",
columns = 3,
points = list(pch = super.sym$pch[1:3],
col = super.sym$col[1:3]),
text = list(c("Setosa", "Versicolor", "Virginica"))))
splom(~iris[1:3]|Species, data = iris,
layout=c(2,2), pscales = 0,
varnames = c("Sepal\nLength", "Sepal\nWidth", "Petal\nLength"),
page = function(...) {
ltext(x = seq(.6, .8, length.out = 4),
y = seq(.9, .6, length.out = 4),
labels = c("Three", "Varieties", "of", "Iris"),
cex = 2)
})
splom(~iris[1:3]|Species, data = iris,
layout=c(2,2), pscales = 0,
varnames = c("Sepal\nLength", "Sepal\nWidth", "Petal\nLength"),
page = function(...) {
ltext(x = seq(.6, .8, length.out = 4),
y = seq(.9, .6, length.out = 4),
labels = c("Three", "Varieties", "of", "Iris"),
cex = 2)
})
parallelplot(~iris[1:4] | Species, iris)
pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species",
pch = 21, bg = c("red", "green3", "blue")[unclass(iris$Species)])
pairs(iris[-5], log = "xy") # plot all variables on log scale
pairs(iris, log = 1:4, # log the first four
main = "Lengths and Widths in [log]", line.main=1.5, oma=c(2,2,3,2))
pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species",
pch = 21, bg = c("red", "green3", "blue")[unclass(iris$Species)])
pairs(iris[-5], log = "xy") # plot all variables on log scale
pairs(iris, log = 1:4, # log the first four
main = "Lengths and Widths in [log]", line.main=1.5, oma=c(2,2,3,2))
## formula method
pairs(~ Fertility + Education + Catholic, data = swiss,
subset = Education < 20, main = "Swiss data, Education < 20")
pairs(USJudgeRatings)
## show only lower triangle (and suppress labeling for whatever reason):
pairs(USJudgeRatings, text.panel = NULL, upper.panel = NULL)
## put histograms on the diagonal
panel.hist <- function(x, ...)
{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(usr[1:2], 0, 1.5) )
h <- hist(x, plot = FALSE)
breaks <- h$breaks; nB <- length(breaks)
y <- h$counts; y <- y/max(y)
rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}
pairs(USJudgeRatings[1:5], panel = panel.smooth,
cex = 1.5, pch = 24, bg = "light blue",
diag.panel = panel.hist, cex.labels = 2, font.labels = 2)
## put (absolute) correlations on the upper panels,
## with size proportional to the correlations.
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(0, 1, 0, 1))
r <- abs(cor(x, y))
txt <- format(c(r, 0.123456789), digits = digits)[1]
txt <- paste0(prefix, txt)
if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
text(0.5, 0.5, txt, cex = cex.cor * r)
}
pairs(USJudgeRatings, lower.panel = panel.smooth, upper.panel = panel.cor)
data("iris")
head(iris)
library(ggplot2)
library(e1071)
qplot(Petal.Length, Petal.Width, data=iris, color = species)
qplot(Petal.Length, Petal.Width, data=iris, color = Species)
library(ggplot2)
library(e1071)
svm_model1 <- svm(Species~., data=iris)
summary(svm_model1)
plot(svm_model1, data=iris, Petal.Width~Petal.Length,
slice=list(Sepal.Width=3, Sepal.Length=4))
pred1 <- predict(svm_model1, iris)
table1 <- table(Predicted = pred1, Actual = iris$Species)
table
table1
model1_accuracy = sum(diag(table1))/sum(table1)
model1_accuracy
model1_error <- 1 - model1_accuracy
model1_error
svm_model2 <- svm(Species~., data=iris, kernel = "linear")
summary(svm_model2)
summary(svm_model2)
plot(svm_model2, data=iris, Petal.Width~Petal.Length,
slice=list(Sepal.Width=3, Sepal.Length=4))
pred2 <- predict(svm_model2, iris)
table2 <- table(Predicted = pred2, Actual = iris$Species)
table2
model2_accuracy <- sum(diag(table2))/sum(table2)
model2_accuracy
model2_error <- 1 - model2_accuracy
model2_error
svm_model3 <- svm(Species~., data=iris, kernel = "polynomial")
summary(svm_model3)
plot(svm_model3, data=iris, Petal.Width~Petal.Length,
slice=list(Sepal.Width=3, Sepal.Length=5))
plot(svm_model3, data=iris, Petal.Width~Petal.Length,
slice=list(Sepal.Width=3, Sepal.Length=4))
pred3 <- predict(svm_model3, iris)
table3 <- table(Predicted = pred3, Actual = iris$Species)
summary(svm_model1)
summary(svm_model2)
table3 <- table(Predicted = pred3, Actual = iris$Species)
table3
model3_accuracy <- sum(diag(table3))/sum(table3)
model3_accuracy
model3_error <- 1 - model3_accuracy
model3_error
help(svm)
coef(svm_model2)
coef(svm_model1)
#Working on Laptop
setwd("C:/Users/Naran/Data-Analytics-Project-6000/Data Analytics Project/Income Data")
Income_data_ACS_2010 <- read.csv("ACSST1Y2010.S1901-Data.csv", skip=1)
Income_data_ACS_2011 <- read.csv("ACSST1Y2011.S1901-Data.csv", skip=1)
Income_data_ACS_2012 <- read.csv("ACSST1Y2012.S1901-Data.csv", skip=1)
Income_data_ACS_2013 <- read.csv("ACSST1Y2013.S1901-Data.csv", skip=1)
Income_data_ACS_2014 <- read.csv("ACSST1Y2014.S1901-Data.csv", skip=1)
Income_data_ACS_2015 <- read.csv("ACSST1Y2015.S1901-Data.csv", skip=1)
Income_data_ACS_2016 <- read.csv("ACSST1Y2016.S1901-Data.csv", skip=1)
Income_data_ACS_2017 <- read.csv("ACSST1Y2017.S1901-Data.csv", skip=1)
Income_data_ACS_2018 <- read.csv("ACSST1Y2018.S1901-Data.csv", skip=1)
Income_data_ACS_2019 <- read.csv("ACSST1Y2019.S1901-Data.csv", skip=1)
Income_data_ACS_2021 <- read.csv("ACSST1Y2021.S1901-Data.csv", skip=1)
View(Income_data_ACS_2011)
summary(Income_data_ACS_2010)
View(Income_data_ACS_2014)
library(MASS)
library(tree)
head(Boston)
set.seed(1)
head(Boston)
train = sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston = tree(medv ~., Boston, subset = train)
summary(tree.boston)
tree(formula = medv~., data = Boston, subset = train)
plot(tree.boston)
text(tree.boston, pretty = 0)
cv.boston = cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, typ = 'b')
prune.boston = prune.tree(tree.boston, best = 5)
plot(prune.boston)
test(prune.boston, pretty = 0)
text(prune.boston, pretty = 0)
yhat = predict(tree.boston, newdata = Boston[-train ,])
boston.test = Boston[-train, "medv"]
plot(yhat, boston.test())
plot(yhat, boston.test
plot(yhat, boston.test)
plot(yhat, boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
#Bagging
library(randomForest)
set.seed(1)
bag.boston = randomForest(medv ~., data = Boston, subset = train,
mtry = 13, importance = TRUE)
bag.boston
yhat.bag = predict(bag.boston, newdata = Boston[-train ,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag - boston.test)^2)
bag.boston = randomForest(medv~., data = Boston,
subset = train, mtry = 13, ntree = 25)
yhat.bag = predict(bag.boston, newdata = Boston[-train ,])
mean((yhat.bag - boston.test)^2)
rfBoston <- randomForest(medv~., data = Boston, subset = train,
mtry = 6, importance = TRUE)
yhat.rf = predict(rfBoston, newdata = Boston[-train ,])
mean((yhat.rf - boston.test)^2)
importance(rfBoston)
varlmPlot(rfBoston)
varlmpPlot(rfBoston)
varlmpPlot(rfBoston)
